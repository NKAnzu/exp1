{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d08ee4",
   "metadata": {},
   "source": [
    "<H2>6. 작사가 인공지능 만들기</h2>\n",
    "이번 노드에서는 다음의 문항을 기준으로 수행하겠다.\n",
    "\n",
    "1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가?\n",
    "\n",
    "- 특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가?\n",
    "\n",
    "2. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
    "\n",
    "- 텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
    "\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
    "\n",
    "- 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3037eb0",
   "metadata": {},
   "source": [
    "목표 : 각 소설의 문장들을 통해 새로운 문장을 모델을 통해 \"생성\"하는 것 \n",
    "\n",
    "수행해야할 과정 : 각 소설의 문장들을 가져와서 정제하고, 모델에 넣어서 학습하고, 예측하듯이 새로운 문장을 \"생성\"한다.\n",
    "\n",
    "루브릭\n",
    "\n",
    "- 데이터 전처리시, 특수문자 제거, tokenizer 생성, padding 처리 \n",
    "\n",
    "- textGenerator로 생성된 문장이 우리눈으로 보기에 해석가능한가 \n",
    "\n",
    "-  생성모델이 안정적으로 학습되었는지 (val loss 2.2이하) - train과정에서 validation 비율을 주고 plot을 통해 시각화하자.\n",
    "\n",
    "\n",
    "\n",
    "목차 \n",
    "\n",
    "- 데이터 다운로드\n",
    "\n",
    "- 데이터 읽어오기 : raw_corpus에 각 줄별로 저장.\n",
    " \n",
    "- 데이터 정제 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782dd0a9",
   "metadata": {},
   "source": [
    "먼저 lyrics 파일에있는 소설의 txt파일들을 가져와서 다룰 것이다.\n",
    "\n",
    "파일들을 glob을 통해 가져온 다음에는, 각 splitlines별로 raw_corpus에 추가해준다.\n",
    "\n",
    "데이터 그 자체들은 raw_corpus에 저장되게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5555c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e8324",
   "metadata": {},
   "source": [
    "Step 1. 데이터 다운로드 /aiffel/lyricist/data/lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe1c6d",
   "metadata": {},
   "source": [
    "Step 2. 데이터 읽어오기 glob을 통해 읽어온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d9729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기 :  187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv(\"HOME\")+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = []\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file,\"r\")as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "    \n",
    "print(\"데이터 크기 : \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408e2b8",
   "metadata": {},
   "source": [
    "Step 3. 데이터 정제\n",
    "\n",
    "입력된 문장을\n",
    "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "5. 다시 양쪽 공백을 지웁니다\n",
    "6. 문장 시작에는 < start >, 끝에는 < end >를 추가합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c6b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence <end>\n"
     ]
    }
   ],
   "source": [
    "#데이터 정제하는 함수 preprocess_sentence\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1 소문자로 바꾸고 양쪽공백 제거\n",
    "    sentence = re.sub(r\"([?.!,¿])\",r\"\\1\", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+',\" \",sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)#4\n",
    "    sentence = sentence.strip() #5\n",
    "    sentence = '<start> ' + sentence +' <end>'#6\n",
    "    return sentence\n",
    "print(preprocess_sentence(\"This @_is ;;; sample      sentence\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de4abd",
   "metadata": {},
   "source": [
    "정제된 문장을 corpus에 모으는데, \n",
    "\n",
    "조건으로 \n",
    "\n",
    "- 공백인 문장, \n",
    "\n",
    "- 마지막 단어가:인 문장,\n",
    "\n",
    "- 단어 토큰이 15개이상인 문장, \n",
    "\n",
    "이 문장들은 학습데이터에서 제외된다.\n",
    "\n",
    "그리고 선별된 문장들을 앞의 전처리 함수를 이용해 문장을 정제한 뒤, corpus에 append해서 모은다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb1ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기에 정제된 문장을 모은다\n",
    "corpus = []\n",
    "max_len=15\n",
    "#raw_corpus_list에 저장된 문장들 순서대로 반환하여 sentence에 저장한다.\n",
    "#문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다.\n",
    "for sentence in raw_corpus:\n",
    "    #우리가 원하지 않는 문장 패스\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1]==':':continue\n",
    "    if len(sentence) >=15: continue\n",
    "        \n",
    "    #앞서 구현한 함수 prerocess_sentence() 함수를 이용하여 문장을 정제, 담는다\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8629d8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> la da da da <end>',\n",
       " '<start> hey, my bobby <end>',\n",
       " '<start> hey, my bobby <end>',\n",
       " '<start> oh yes indeed. <end>',\n",
       " '<start> la da da da <end>',\n",
       " '<start> hey, my bobby <end>',\n",
       " '<start> hey, my bobby <end>',\n",
       " '<start> oh yes indeed. <end>',\n",
       " '<start> i promise. <end>',\n",
       " '<start> whoa babe, <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a727e",
   "metadata": {},
   "source": [
    "사용할 문장들을 모았다면 다음엔 토큰화를 시켜야한다.\n",
    "\n",
    "단어 12000개로 제한하고, 텍스트->토큰화를 진행한다\n",
    "\n",
    "fit_on_texts를 통해 corpus의 문장들을 리스트의 형태로 변환한다.\n",
    "\n",
    "texts_to_sequences를 통해 문장들의  단어들을 숫자 시퀀스 형태로 변환한다.\n",
    "\n",
    "시퀀스들의 길이를 일정하게 맞추기위해 패딩을 붙힌다.\n",
    "\n",
    "그리고 tensor와 tokenizer을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66576c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   46   27 ...    0    0    0]\n",
      " [   2  129   11 ...    0    0    0]\n",
      " [   2  129   11 ...    0    0    0]\n",
      " ...\n",
      " [   2   92    3 ...    0    0    0]\n",
      " [   2 3543    3 ...    0    0    0]\n",
      " [   2    3    0 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f8b49462310>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    #7000단어 기억하는 tokenizer만들것\n",
    "    #문장은 이미 정제되서 filter필요없다\n",
    "    #7000단어에 포함안되는 단어는 <unk>\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                num_words=12000,\n",
    "                filters=' ',\n",
    "                oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d94cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae5b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer.index_word) #말그대로 토크나이저의 단어장이다. dict모양으로  index : 'word' 형태로 되어있다.\n",
    "print(type(tokenizer.index_word[1])) \n",
    "print(tokenizer.index_word[1]) # 7 : 'oh' 의 데이터다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47741d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : you\n",
      "6 : it\n",
      "7 : oh\n",
      "8 : me\n",
      "9 : a\n",
      "10 : the\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word: 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형대로 반환 (Ex. {index: '~~', index: '~~', ...})\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c6f4b",
   "metadata": {},
   "source": [
    "Step 4. 평가 데이터셋 분리\n",
    "\n",
    "train : val : test = 6 : 2 : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f4adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:,:-1]\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da5b81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_input)#12147,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b478e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12147"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_input)#12147,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0744af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_rem, y_train, y_rem = train_test_split(src_input, tgt_input, test_size=0.4, random_state=42)\n",
    "x_val, x_test, y_val, y_test =  train_test_split(x_rem, y_rem, test_size=0.5, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef1823",
   "metadata": {},
   "source": [
    "나중에 model.fit에 사용할 train과 val data들을 각각 텐서로 만들고 배치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672d4587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7288, 8)\n",
      "(7288, 8)\n",
      "<BatchDataset shapes: ((256, 8), (256, 8)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 8), (256, 8)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 num_words를 7000개로 선언했기 때문에, tokenizer.num_words의 값은 7000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ecf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다! \n",
    "\n",
    "embedding_size=256  # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
    "hidden_size=1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4227de",
   "metadata": {},
   "source": [
    "TextGenrator을 통해 model에 넣습니다. (모델 층 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "877af001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 8, 12001), dtype=float32, numpy=\n",
       "array([[[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [-1.52461958e-04, -3.90336616e-04,  1.78138434e-04, ...,\n",
       "          2.22294242e-04,  3.62141727e-04,  2.09300022e-04],\n",
       "        [-3.24215012e-04, -5.92029712e-04,  4.29409934e-04, ...,\n",
       "          1.45138561e-04,  4.59116738e-04,  1.49460131e-04],\n",
       "        ...,\n",
       "        [-5.17292283e-05, -2.00751907e-04,  6.74095529e-04, ...,\n",
       "          1.52885550e-05,  7.92512961e-04, -1.60317533e-04],\n",
       "        [-1.57222836e-04,  2.24211341e-04,  4.35719034e-04, ...,\n",
       "         -1.45922342e-04,  6.51543436e-04,  8.32149453e-05],\n",
       "        [-3.18861363e-04,  6.63321873e-04,  2.32218605e-04, ...,\n",
       "         -3.02185246e-04,  4.59815783e-04,  4.08117252e-04]],\n",
       "\n",
       "       [[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [-5.78567960e-05, -2.84684447e-05, -3.26278023e-05, ...,\n",
       "          1.86871956e-04,  5.27137192e-04, -7.20724638e-05],\n",
       "        [-6.24831785e-07, -5.26547092e-06, -1.88456339e-04, ...,\n",
       "          2.87299161e-04,  6.49992609e-04, -3.93556926e-04],\n",
       "        ...,\n",
       "        [-4.40203032e-04,  8.75001831e-04, -6.92053873e-04, ...,\n",
       "         -1.61354677e-04,  2.90756201e-04,  2.64131959e-04],\n",
       "        [-6.34146272e-04,  1.18153542e-03, -7.01228215e-04, ...,\n",
       "         -3.32950411e-04,  1.11374640e-04,  6.23868196e-04],\n",
       "        [-8.19005072e-04,  1.44316140e-03, -6.35151693e-04, ...,\n",
       "         -4.79031965e-04, -5.46834017e-05,  9.66192863e-04]],\n",
       "\n",
       "       [[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [-1.70754574e-04, -2.91806908e-04,  7.07461222e-05, ...,\n",
       "          2.59739463e-05,  6.35204546e-04,  1.60066076e-04],\n",
       "        [-1.80445568e-04, -3.08871618e-04, -5.25048235e-05, ...,\n",
       "          1.45946251e-04,  9.06221452e-04,  1.98116817e-04],\n",
       "        ...,\n",
       "        [-1.91809566e-04, -1.25725899e-06, -4.06564068e-04, ...,\n",
       "         -9.63722559e-05,  9.57490120e-04,  2.53502774e-04],\n",
       "        [-2.78583932e-04,  3.63828411e-04, -4.66938072e-04, ...,\n",
       "         -2.77061888e-04,  7.29846710e-04,  5.75305312e-04],\n",
       "        [-4.03793703e-04,  7.40035146e-04, -4.56063281e-04, ...,\n",
       "         -4.49886953e-04,  4.72399493e-04,  9.39182297e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [ 2.11116185e-05,  2.10799408e-04,  3.58804013e-04, ...,\n",
       "          1.66741476e-04,  5.90335636e-04, -3.04618647e-04],\n",
       "        [-1.63511853e-04,  2.58406246e-04,  5.77715633e-04, ...,\n",
       "          8.29528381e-06,  4.39745432e-04, -6.06678717e-04],\n",
       "        ...,\n",
       "        [-4.22829413e-04,  5.82101289e-04,  2.06213095e-04, ...,\n",
       "         -3.80919926e-04,  4.19280768e-05, -6.55123324e-04],\n",
       "        [-5.91826392e-04,  8.64921196e-04,  5.27085576e-05, ...,\n",
       "         -5.57376537e-04, -1.11538757e-04, -2.71336030e-04],\n",
       "        [-7.61517265e-04,  1.14322780e-03, -3.45137269e-05, ...,\n",
       "         -7.01035664e-04, -2.55081774e-04,  1.40274118e-04]],\n",
       "\n",
       "       [[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [-6.99593729e-05, -4.46068996e-04,  1.93230706e-04, ...,\n",
       "          4.61395743e-04,  3.71888251e-04,  2.09891121e-04],\n",
       "        [ 1.65786823e-05, -6.18999649e-04,  2.14138796e-04, ...,\n",
       "          6.93985145e-04,  4.36231989e-04,  8.87149145e-05],\n",
       "        ...,\n",
       "        [-4.18230135e-04,  2.72212492e-04, -8.29473429e-05, ...,\n",
       "          2.53836479e-04,  6.94750124e-05,  9.27383662e-04],\n",
       "        [-6.45458815e-04,  6.56289107e-04, -1.01640566e-04, ...,\n",
       "          3.56198070e-05, -1.04481325e-04,  1.25926337e-03],\n",
       "        [-8.70282063e-04,  1.00102008e-03, -6.64434701e-05, ...,\n",
       "         -1.62355660e-04, -2.66092480e-04,  1.55384245e-03]],\n",
       "\n",
       "       [[-2.91267825e-05, -1.19322998e-04,  3.92294278e-05, ...,\n",
       "          1.21089317e-04,  1.74979257e-04,  9.30213573e-05],\n",
       "        [-1.90134873e-04, -3.32511991e-05,  4.92646759e-05, ...,\n",
       "          1.08848028e-04, -3.16499172e-05,  2.22126953e-04],\n",
       "        [-2.91543169e-04,  1.10070978e-05, -1.41328623e-04, ...,\n",
       "         -7.79964175e-05, -4.87356418e-04,  2.05418532e-04],\n",
       "        ...,\n",
       "        [ 1.91366216e-05,  1.83908178e-05, -6.53071038e-04, ...,\n",
       "          3.41170613e-04, -1.05910178e-03,  4.18020907e-04],\n",
       "        [-6.71695234e-05,  2.91282893e-04, -7.52285647e-04, ...,\n",
       "          2.22118877e-04, -1.17470941e-03,  6.75922609e-04],\n",
       "        [-2.15499429e-04,  6.07587688e-04, -7.65874109e-04, ...,\n",
       "          5.22291775e-05, -1.26788847e-03,  9.76211391e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a538de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련데이터와 테스트데이터 분리 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57911947",
   "metadata": {},
   "source": [
    "Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac83c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Loss\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe689f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = loss, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ee9e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 8), (256, 8)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd214a40",
   "metadata": {},
   "source": [
    "모델을 학습시키고 저장한다 (매번 학습시키기에는 시간이 오래걸리기 때문이다.)\n",
    "\n",
    "근데 수행을 해보니 h5파일로 만들려면 함수형 모델이나 순차형 모델이여야 한다고 한다. 안전하게 serialize화도 불가능.\n",
    "\n",
    "Tensorflow SavedModel로 해보라는 듯 하다.\n",
    "\n",
    "NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not \n",
    "\n",
    "work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable.\n",
    "\n",
    "Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
    "\n",
    "오류 힌트를 통해 model.save 뒤에 save_format='tf'를 지정한다\n",
    "\n",
    "------\n",
    "\n",
    "이렇게 수행하였으나 다시 오류가 발생했기에 save만 해보고 load는 하지않는걸로 넘어가겠다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b0260e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "28/28 - 6s - loss: 4.3222 - val_loss: 2.8621\n",
      "Epoch 2/30\n",
      "28/28 - 3s - loss: 2.4927 - val_loss: 2.4483\n",
      "Epoch 3/30\n",
      "28/28 - 3s - loss: 2.1475 - val_loss: 2.2008\n",
      "Epoch 4/30\n",
      "28/28 - 3s - loss: 1.9630 - val_loss: 2.0988\n",
      "Epoch 5/30\n",
      "28/28 - 3s - loss: 1.8840 - val_loss: 2.0623\n",
      "Epoch 6/30\n",
      "28/28 - 3s - loss: 1.8379 - val_loss: 2.0428\n",
      "Epoch 7/30\n",
      "28/28 - 3s - loss: 1.8120 - val_loss: 2.0349\n",
      "Epoch 8/30\n",
      "28/28 - 3s - loss: 1.7909 - val_loss: 2.0337\n",
      "Epoch 9/30\n",
      "28/28 - 3s - loss: 1.7761 - val_loss: 2.0297\n",
      "Epoch 10/30\n",
      "28/28 - 3s - loss: 1.7564 - val_loss: 2.0227\n",
      "Epoch 11/30\n",
      "28/28 - 3s - loss: 1.7357 - val_loss: 2.0131\n",
      "Epoch 12/30\n",
      "28/28 - 3s - loss: 1.7186 - val_loss: 2.0050\n",
      "Epoch 13/30\n",
      "28/28 - 3s - loss: 1.6956 - val_loss: 2.0017\n",
      "Epoch 14/30\n",
      "28/28 - 3s - loss: 1.6737 - val_loss: 1.9770\n",
      "Epoch 15/30\n",
      "28/28 - 3s - loss: 1.6485 - val_loss: 1.9720\n",
      "Epoch 16/30\n",
      "28/28 - 3s - loss: 1.6247 - val_loss: 1.9585\n",
      "Epoch 17/30\n",
      "28/28 - 3s - loss: 1.6054 - val_loss: 1.9486\n",
      "Epoch 18/30\n",
      "28/28 - 3s - loss: 1.5833 - val_loss: 1.9434\n",
      "Epoch 19/30\n",
      "28/28 - 3s - loss: 1.5646 - val_loss: 1.9376\n",
      "Epoch 20/30\n",
      "28/28 - 3s - loss: 1.5455 - val_loss: 1.9308\n",
      "Epoch 21/30\n",
      "28/28 - 3s - loss: 1.5315 - val_loss: 1.9285\n",
      "Epoch 22/30\n",
      "28/28 - 3s - loss: 1.5165 - val_loss: 1.9229\n",
      "Epoch 23/30\n",
      "28/28 - 3s - loss: 1.4936 - val_loss: 1.9139\n",
      "Epoch 24/30\n",
      "28/28 - 3s - loss: 1.4774 - val_loss: 1.9096\n",
      "Epoch 25/30\n",
      "28/28 - 3s - loss: 1.4654 - val_loss: 1.9093\n",
      "Epoch 26/30\n",
      "28/28 - 3s - loss: 1.4492 - val_loss: 1.9096\n",
      "Epoch 27/30\n",
      "28/28 - 3s - loss: 1.4333 - val_loss: 1.8986\n",
      "Epoch 28/30\n",
      "28/28 - 3s - loss: 1.4184 - val_loss: 1.8979\n",
      "Epoch 29/30\n",
      "28/28 - 3s - loss: 1.4058 - val_loss: 1.9022\n",
      "Epoch 30/30\n",
      "28/28 - 3s - loss: 1.3950 - val_loss: 1.8921\n"
     ]
    }
   ],
   "source": [
    "# 한번만 실행하자 오래걸리므로, 아래에 h5파일 load_model을 통해 predict 테스트 할수 있게 따로 마련해 놓았다.\n",
    "history = model.fit(dataset, validation_data=val_dataset, epochs=30, verbose=2) #val_dataset 또한 fit해서 val_loss를 본다.\n",
    "\n",
    "#from keras.models import load_model\n",
    "#model.save('exp6 model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32185e45",
   "metadata": {},
   "source": [
    "validation loss값이 2.2이하로 낮아졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54fd46b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(history.history))\n",
    "plt.plot(history.history['loss'],'r',history.history['val_loss'],'b')\n",
    "plt.legend('best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1382912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5 파일을 불러와서 model에 적용, 이를 통해 텍스트 생성을 진행한다\n",
    "#from keras.models import load_model\n",
    "#model = load_model('exp6 model')\n",
    "#아래 generate_text에서 제대로 수행되지 않기에 주석처리하고 넘어가겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad5ee5",
   "metadata": {},
   "source": [
    "gernerate_text, 문장생성 함수에서 수행하는것은 입력받은 단어를 시퀀스로 변환한 뒤, 텐서로 변환합니다.\n",
    "\n",
    "그리고 마지막 토큰 end_token에는 < end >의 index를 저장합니다\n",
    "\n",
    "단어 하나씩 예측해서 문장을 만드는 구조이고 Embedding LSTM LSTM linear\n",
    "\n",
    "1. 입력받은 문장의 텐서를 입력 \n",
    "\n",
    "2. 예측된 값중에 가장 높은 확률인 word의 index를 뽑아냅니다. tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "\n",
    "3. 2에서 예측된 word의 index를 test_tensor 문장 뒤에 붙입니다.\n",
    "\n",
    "이후 1,2,3을 반복하다가 \n",
    "\n",
    "4. 예측된 단어가 end_token이라면 탈출, test_tensor의 길이가 max_len보다 길면 탈출합니다.\n",
    "\n",
    "최종적으로 tokenizer을 이용해 word_index를 단어로 하나씩 변환합니다!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899b28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): \n",
    "    #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b91ff9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love \", max_len=20)\n",
    "# generate_text 함수에 lyricist 라 정의한 모델을 이용해서 ilove 로 시작되는 문장을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd81a1d",
   "metadata": {},
   "source": [
    " <h2>회고</h2>\n",
    " \n",
    " 여러가지 과정을 거쳐서 진행을 해보려했지만 안되었던 것이 많았던것 같다\n",
    " \n",
    " 우선 학습이 오래걸리는 모델을 저장해서 load_model을 통해 불러와 텍스트생성을 해보려했지만\n",
    " \n",
    " 오류가 떠서 주석처리를 해 보류하였다.\n",
    " \n",
    " 그리고 하나 잘못생각했던게 있는것같은게 생성모델의 정확성이라던가는 컴퓨터가 판단하는게 이상한데, accuracy를 검사해보려 헀던것이다\n",
    " \n",
    " 사람이 생성된 문장을 보고 이상함을 느끼는 주관적인 판단이 곧 이 생성모델의 wrong이 아닐까 싶다.\n",
    " \n",
    " 일단 지식이 얕아 이렇게까지 알아보겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
